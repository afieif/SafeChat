{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNKCeZQYeEhILmR4mRPrqM4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Installation and imports"],"metadata":{"id":"DCPAe1R-mCPS"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84ZJ_Wqzno9t","outputId":"d56dc4f0-ca60-4831-f81a-ac6c72fbb283"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install transformers --quiet"]},{"cell_type":"code","source":["import os\n","import tensorflow as tf\n","import transformers\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import math\n","from matplotlib import pyplot as plt"],"metadata":{"id":"QeYSo0AaoFW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"yyXKhYgLoJtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT_DIR = \"/content/drive/MyDrive/Datasets\"\n","OUTPUT_DIR = \"/content/drive/MyDrive/Datasets\"\n","CSV_FILE_PATH = os.path.join(INPUT_DIR, \"paradetox.csv\")\n","LOG_FILE_PATH = os.path.join(INPUT_DIR, \"training_log.csv\")\n","CHECKPOINT_FILE_PATH = os.path.join(OUTPUT_DIR, \"toxicity_model_epoch_{epoch:02d}.h5\")"],"metadata":{"id":"9hAvyFKyoldc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Restructure Dataset"],"metadata":{"id":"HAjs7NBvmHl0"}},{"cell_type":"code","source":["dataset = pd.read_csv(CSV_FILE_PATH)\n","\n","toxic = pd.DataFrame(columns=[\"comment\", \"toxicity\"])\n","toxic['comment'] = dataset['toxic']\n","toxic['toxicity'] = 1\n","\n","\n","neutral1 = pd.DataFrame(columns=[\"comment\", \"toxicity\"])\n","neutral1['comment'] = dataset['neutral1']\n","neutral1['toxicity'] = 0\n","\n","neutral2 = pd.DataFrame(columns=[\"comment\", \"toxicity\"])\n","neutral2['comment'] = dataset['neutral2']\n","neutral2['toxicity'] = 0\n","\n","neutral3 = pd.DataFrame(columns=[\"comment\", \"toxicity\"])\n","neutral3['comment'] = dataset['neutral3']\n","neutral3['toxicity'] = 0\n","\n","data = toxic.append(neutral1.append(neutral2.append(neutral3)))\n"],"metadata":{"id":"1iGzR1oFpApO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data.dropna()\n","data.head(-5)"],"metadata":{"id":"nhYqgnuYy8ms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = data[\"comment\"]\n","Y = data[\"toxicity\"]"],"metadata":{"id":"PyO0Tmbe4bk5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train , Test and Validation Splitting"],"metadata":{"id":"LPlpI_I-mMer"}},{"cell_type":"code","source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, shuffle=True, random_state=63)\n","X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.50, shuffle=True, random_state=63)\n","\n","print(\"Train size: \", len(X_train))\n","print(\"Validation size: \", len(X_val))\n","print(\"Test size: \", len(X_test))"],"metadata":{"id":"NleA_kbA4bqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")"],"metadata":{"id":"jpSo_RTx45AR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tokenize the data\n"],"metadata":{"id":"VRWHRPsnmgWu"}},{"cell_type":"code","source":["X_train = tokenizer(\n","            text=X_train[:].tolist(),\n","            padding='max_length',\n","            return_tensors='np',\n","            truncation=True,\n","            return_token_type_ids=False,\n","            return_attention_mask=True\n","        )\n","X_val = tokenizer(\n","            text=X_val[:].tolist(),\n","            padding='max_length',\n","            return_tensors='np',\n","            truncation=True,\n","            return_token_type_ids=False,\n","            return_attention_mask=True\n","        )\n","X_test = tokenizer(\n","            text=X_test[:].tolist(),\n","            padding='max_length',\n","            return_tensors='np',\n","            truncation=True,\n","            return_token_type_ids=False,\n","            return_attention_mask=True\n","        )"],"metadata":{"id":"iYz7thq05BpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_train = Y_train[:].to_numpy()\n","Y_val = Y_val[:].to_numpy()\n","Y_test = Y_test[:].to_numpy()"],"metadata":{"id":"vJz5AYTj6m24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DataGenerator(tf.keras.utils.Sequence):\n","\n","    def __init__(self, text, target, batch_size, shuffle=False, step_size=None):\n","      self.text = text\n","      self.target = target\n","      self.batch_size = batch_size\n","      self.shuffle = shuffle\n","      self.step_size = step_size\n","      self.indices = np.arange(len(self.text['input_ids']))\n","\n","    def __len__(self):\n","        if self.step_size:\n","            return self.step_size\n","        return math.ceil(len(self.text['input_ids']) / self.batch_size)\n","\n","    def __getitem__(self, idx):\n","        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        text_ids = self.text['input_ids'][inds]\n","        text_masks = self.text['attention_mask'][inds]\n","\n","        target = self.target[inds]\n","\n","        return [text_ids, text_masks], target\n","\n","    def on_epoch_end(self):\n","        if self.shuffle:\n","            np.random.shuffle(self.indices)"],"metadata":{"id":"_SQPFZMz6tIM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_gen = DataGenerator(X_train, Y_train, batch_size=16,\n","                          shuffle=True, step_size=None)\n","val_gen = DataGenerator(X_val, Y_val, batch_size=16,\n","                        shuffle=False, step_size=None)\n","test_gen = DataGenerator(X_test, Y_test, batch_size=16,\n","                         shuffle=False, step_size=None)"],"metadata":{"id":"rJjLOorL60YN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model(bert_model_name, max_length, dropout_rate, num_classes, activation, model_name):\n","\n","    # Encoded token ids from BERT tokenizer.\n","    text_input = tf.keras.layers.Input(\n","        shape=(max_length,),\n","        dtype=tf.int32,\n","        name=\"input_ids\"\n","    )\n","    # Attention masks indicates to the model which tokens should be attended to.\n","    text_mask = tf.keras.layers.Input(\n","        shape=(max_length,),\n","        dtype=tf.int32,\n","        name=\"attention_masks\"\n","    )\n","\n","    # If using the compact models, need to convert them from Pytorch versions\n","    if \"bert_uncased\" in bert_model_name:\n","      bert_model = transformers.TFBertModel.from_pretrained(bert_model_name,\n","                                                            from_pt=True)\n","    else:\n","      bert_model = transformers.TFBertModel.from_pretrained(bert_model_name)\n","\n","    dropout = tf.keras.layers.Dropout(\n","        rate=dropout_rate,\n","        name=\"dropout\"\n","    )\n","\n","    linear = tf.keras.layers.Dense(\n","        num_classes,\n","        activation=activation,\n","        name=\"classifier\"\n","    )\n","\n","    # designing the model architecture\n","    bert_output = bert_model.bert([text_input, text_mask])\n","    pooler_output = bert_output.pooler_output\n","\n","    dropout_output = dropout(pooler_output)\n","\n","    result = linear(dropout_output)\n","\n","    model = tf.keras.models.Model(\n","        inputs=[text_input, text_mask],\n","        outputs=result,\n","        name=model_name\n","    )\n","\n","    model.build(\n","        input_shape=(\n","            (None, max_length),\n","            (None, max_length),\n","        )\n","    )\n","\n","    return model"],"metadata":{"id":"z2I4dPs563la"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Loading model...\")\n","model = build_model(\"google/bert_uncased_L-12_H-512_A-8\", 64, 0.5, 1, \"sigmoid\", \"DemoModel\")\n","\n","# When finetuning using BERT, recommend to put a small learning rate (1e-5, 2e-5, 3e-5, etc)\n","# We use binary_crossentropy as loss since we are performing binary classification\n","model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), metrics=[\"accuracy\"])\n","\n","model.summary()\n","print(\"Model loaded.\")"],"metadata":{"id":"uxEmHDdd6_6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=CHECKPOINT_FILE_PATH,\n","    verbose=1,\n","    save_freq=3*len(train_gen)\n",")\n","\n","history_logger = tf.keras.callbacks.CSVLogger(LOG_FILE_PATH,  separator=\",\", append=True)\n","\n","callbacks_list = [model_checkpoint, history_logger]"],"metadata":{"id":"JGzoL53gCudF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train model\n","print('Training started...')\n","history = model.fit(\n","    train_gen,\n","    validation_data=val_gen,\n","    epochs=3,\n","    initial_epoch=0,\n","    verbose=1,\n","    callbacks=callbacks_list\n",")\n","print(\"Training done.\")"],"metadata":{"id":"2CGndAZ0CvE9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluate the model at end of training\n","model.evaluate(test_gen, verbose=1)"],"metadata":{"id":"S-ykyRl2CzY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def classify(comment):\n","  tokenized_text = tokenizer(\n","              text=comment,\n","              padding='max_length',\n","              max_length=64,\n","              return_tensors='np',\n","              truncation=True,\n","              return_token_type_ids=False,\n","              return_attention_mask=True\n","          )\n","\n","  # We extract the probability\n","  y_pred = model.predict([tokenized_text[\"input_ids\"], tokenized_text[\"attention_mask\"]])[0][0]\n","\n","  if y_pred < 0.5:\n","    print(\"Normal\")\n","  else:\n","    print(\"Toxic\")"],"metadata":{"id":"BjR2jsTDCz4g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classify(\"You can't do anything properly, you are a stupid idiot\")"],"metadata":{"id":"HGOflt8DLQW4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('/content/drive/MyDrive/models/toxicity_detector.keras')"],"metadata":{"id":"G6zA-VJ3Mame"},"execution_count":null,"outputs":[]}]}